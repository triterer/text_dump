Глава 3
========================

Глава 3. Гипотезы и архитектурные улучшения

Это самая важная «научная» часть, где появляется распределённость.

    Гипотеза: «Применение протокола Raft для синхронизации Schema Registry и использование двухфазной фиксации (2PC) совместно с иерархическими блокировками позволит обеспечить строгую согласованность при сохранении высокого параллелизма в распределённой среде».

    Новый результат: Ты расширяешь модели CSP и сетей Петри из 2-й главы, добавляя в них сетевое взаимодействие, задержки и отказы узлов.

    Доказательство: Ты математически показываешь, что твоя система остается согласованной (Safety) и работоспособной (Liveness) даже при распределении данных по шардам.

  ---

Чтобы работа соответствовала заявленной теме «распределённая база данных», нам нужно добавить механизмы **горизонтального масштабирования (sharding)**, **репликации** и **согласованности (consensus)**. Без этого система будет восприниматься как локальное хранилище с сетевым интерфейсом.

Вот конкретные архитектурные пласты, которые стоит добавить:

---

### 1. Шардирование по Document ID (Data Partitioning)

Поскольку ты используешь RocksDB (которая является локальным движком), тебе нужно распределить данные между узлами.

* **Метод:** Использование **Consistent Hashing** (последовательного хеширования).
* **Реализация:** Ключ `d:collection_1:document_1` хешируется, и по хешу определяется, на какой конкретной Data Node лежат все записи этого документа.
* **Научный интерес:** Как иерархическая блокировка работает, если запрос затрагивает документы на разных шардах? Здесь появляется задача координации распределенных блокировок.

### 2. Распределенный консенсус для Schema Registry

Schema Registry не может быть «просто прослойкой», иначе это единая точка отказа (SPOF).

* **Реализация:** Сделай Schema Registry **Replicated State Machine** на базе алгоритма **Raft** (или Paxos).
* **Связь с темой:** Это гарантирует, что все узлы видят одну и ту и ту же версию глобальной схемы. При обновлении схемы (добавление нового пути) лидер Raft-группы подтверждает изменение, прежде чем DLM начнет выдавать блокировки по новому пути.

### 3. Distributed Lock Manager (DLM) и Global Deadlocks

В распределенной системе блокировки становятся намного сложнее.

* **Локальный vs Глобальный DLM:** Каждый узел (Data Node) управляет блокировками для своих шардов (локальное Radix-дерево). Но если транзакция хочет обновить два документа на разных узлах, возникает риск **распределенного дедлока**.
* **Метод реализации:** Можно добавить «Координатор транзакций» или использовать алгоритм **Wait-Die / Wound-Wait** для предотвращения циклического ожидания между узлами.

### 4. Протокол распределенной транзакции (2PC)

Если твоя БД поддерживает атомарность (Atomicity), тебе понадобится протокол двухфазной фиксации (2PC) или его оптимизированные варианты.

* **Процесс:**
1. **Prepare:** Координатор запрашивает иерархические блокировки на всех вовлеченных Data Nodes.
2. **Commit/Abort:** Если все узлы подтвердили захват замков и готовность записи, данные фиксируются в RocksDB.


* **Научная новизна:** Описание и верификация через **CSP** взаимодействия между Координатором и Участниками в контексте иерархических блокировок.

---

## Как это изменит описание твоей системы?

Теперь архитектура будет выглядеть так:

1. **Client/Proxy Layer:** Принимает JSON, вычисляет шард документа.
2. **Consensus Layer (Raft):** Хранит Schema Registry и метаданные о распределении шардов.
3. **Data Nodes (Rust + RocksDB + Local DLM):** Хранят части графа документов.
4. **Distributed Transaction Manager:** Координирует захват блокировок на разных узлах.

---

## Что это дает для магистерской работы?

Добавление распределенности позволяет тебе расширить раздел **верификации (CSP и Сети Петри)**:

* **Моделирование сетевых задержек и сбоев:** Ты можешь доказать в CSP, что твоя система не переходит в некорректное состояние, если один из узлов «упал» в момент обновления схемы.
* **Верификация 2PC поверх иерархических блокировок:** Это очень сильный кейс. Сети Петри здесь идеально покажут переходы состояний транзакции: `READY` -> `VOTED` -> `COMMITTED`.

### Пример дополнения в текст:

> «Для обеспечения горизонтальной масштабируемости система реализует распределенное хранение на основе консистентного хеширования. Координация доступа к ресурсам осуществляется через децентрализованный менеджер блокировок (DLM), работающий в связке с протоколом двухфазной фиксации, что позволяет сохранять ACID-свойства при выполнении мелкозернистых операций над частями JSON-документов, распределенных по кластеру».


## Не проработано
lock escalation
dlm создание дерева блокировок по мере появления блокировок
Сортировка на нужном уровне
Откат при удалении пути до захвата
Распределённость
Обеспечение консистентности (уникальность id, соблюдение fk)