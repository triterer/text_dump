Seq2seq
========================

Основная архитектура это две нейронки (чаще всего [RNN](Recurrent%20Neural%20Network%20%28RNN%29.md)) Энкодер - декодер:
Предполагается что в первой части извлекается некоторое знание в виде вектора, которое передается во второй блок.

Красивые видосы о работе есть [тут](https://habr.com/ru/articles/486158/).

Проблема в том что от начала предложения остается мало контекста, то важно для декодера. Как варианты решения это [attention](%D0%92%D0%BD%D0%B8%D0%BC%D0%B0%D0%BD%D0%B8%D0%B5%20%28attention%29.md) или перевернуть предложение.
