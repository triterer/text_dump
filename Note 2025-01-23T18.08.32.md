Note 2025-01-23T18.08.32
========================

## Seq2Seq


### Механизм внимания
Основная идея - связать каждый шаг декодера с энкодером. На уровне математики мы перемножаем матрицу декодера на матрицы энкодеров, получаем скаляры. При помощи softmax мыполучаем из этих скаляров взвешенный вектор, который потом прилепляется к декодеру конкатенацией. И так для каждого декодера. Это позволяет концентрировать внимание на нужных словах.

Строя карту attention можно зачастую увидеть диагональ связных слов.

В случае несовпадения размеров матриц механизма внимания можно использовать другие формулы.

### Self-attention
Внимание не между токенами, а к самому токену.

Доавляем новые термины к эмбеддингу. Три типа векторов: queryes, keys, values.

Последовательность

value - максимально информативная часть слова по логике. queryes, keys - положения слова в предложении для рассчета одного относительно другого. Расчеты обычно далется не по словам, а используются матрицы.

